{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **1. Imports**","metadata":{}},{"cell_type":"code","source":"# Src: https://www.kaggle.com/code/aman10kr/offline-handwritten-text-ocr\n\nimport warnings \nwarnings.filterwarnings('ignore') \n\nimport numpy as np  \nimport pandas as pd  \nimport matplotlib.pyplot as plt  \nimport matplotlib.image as mpimg \nfrom matplotlib import pyplot \nimport seaborn as sns  \nfrom PIL import Image \nfrom glob import glob \nimport os  \nimport tqdm.notebook as tq  \nimport random   \nimport cv2  \n\nimport keras  \nfrom keras.models import Sequential  \nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D  \nfrom tensorflow.keras.layers import BatchNormalization ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Settings**","metadata":{}},{"cell_type":"code","source":"import matplotlib as mpl \n# Custom colors \nclass color:\n    S = '\\033[1m' + '\\033[94m' \n    E = '\\033[0m' \n# Set Matplotlib graph colors \nmy_colors = [\"#16558F\", \"#1583D2\", \"#61B0B7\", \"#ADDEFF\", \"#A99AEA\", \"#7158B7\"] \nprint(color.S+\"Notebook Color Scheme:\"+color.E) \nsns.palplot(sns.color_palette(my_colors)) \nplt.show()  \n# Set Matplotlib graph settings \nsns.set_style(\"white\") \nmpl.rcParams['xtick.labelsize'] = 14 \nmpl.rcParams['ytick.labelsize'] = 14 \nmpl.rcParams['axes.spines.left'] = False \nmpl.rcParams['axes.spines.right'] = False \nmpl.rcParams['axes.spines.top'] = False \nplt.rcParams.update({'font.size': 14}) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Data Exploration**","metadata":{}},{"cell_type":"code","source":"dir1 = '/kaggle/input/english-handwritten-characters-dataset/Img'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_summary_graphs(*dir, num_samples=500):\n    graph_list = {\"Width/Height Ratio\":[],\"Format\":[],\"Width\":[],\"Height\":[]}\n    directories = [*dir]\n    images = sum([glob(os.path.join(i,j), recursive=True) for j in (\"**/*.png\",\"**/*.jpg\",\"**/*.jpeg\",\"**/*.bmp\") for i in directories],[])\n    all_files = sum([glob(os.path.join(i,'**/*.*'), recursive=True) for i in directories],[])\n    graph_list[\"Format\"] = [fp.split('.')[-1] for fp in all_files]\n    print('Total Number of images: %d'%len(images))\n    samples = images if num_samples==0 else random.sample(images, num_samples)\n    for image in samples:\n        im = Image.open(image)\n        graph_list[\"Width\"].append(im.size[0])\n        graph_list[\"Height\"].append(im.size[1])\n        graph_list[\"Width/Height Ratio\"].append(im.size[0]/im.size[1])\n    plt.figure(figsize=(15,10))\n    for i,k in enumerate(graph_list):\n        plt.subplot(2, 2, i+1)\n        # ax = sns.countplot(x=graph_list[k])\n        # ax.bar_label(ax.containers[0])\n        counts, edges, bars = plt.hist(graph_list[k], align=\"mid\")\n        plt.bar_label(bars)\n        # plt.hist(graph_list[k], histtype='stepfilled', align='mid')\n        plt.title('Distribution of '+ k)\n    plt.tight_layout()\n    plt.ticklabel_format(useOffset=False) # Prevents scientific notation (e.g 1.28e3) \n    plt.show()\n\n# set num_samples to 0 to check all dir\nplot_summary_graphs(dir1, num_samples=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images_dirs(*dir, num_samples=5, num_cols=5, plot_gap=6):     \n    directories = [*dir]\n    images = sum([glob(os.path.join(i,j), recursive=True) for j in (\"**/*.png\",\"**/*.jpg\",\"**/*.jpeg\",\"**/*.bmp\") for i in directories],[])\n    samples = images if num_samples==0 else random.sample(images, num_samples)\n    fig = plt.figure(figsize=(20,num_samples/num_cols*plot_gap)) \n    for i,image in enumerate(samples):\n        plt.subplot(round(num_samples/num_cols),num_cols,i+1)   \n        img = Image.open(image)   \n        plt.title(\"Dataset: \" + image.split(\"/\")[-2] + \"\\n\" + str(img.size))    \n        plt.imshow(img)      \n    plt.show()\n\nplot_images_dirs(dir1, num_samples=6, num_cols=3, plot_gap=6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Loading Data**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/english-handwritten-characters-dataset/english.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.array([np.asarray(Image.open('/kaggle/input/english-handwritten-characters-dataset/'+im)) for im in df['image']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['label'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Exploratory Data Analysis**","metadata":{}},{"cell_type":"code","source":"def plot_class_dist(*class_labels):  \n    \"\"\"\n    Plot class distribution\n    class_labels: list or series of class labels\n    \"\"\"\n    fig = plt.figure(figsize=(20, 10))\n    gs = fig.add_gridspec(len(class_labels),1) # Create subplots & specify (#rows, #columns)\n    gs.update(wspace=0.2, hspace=0.2) # Specify spaces between subplot (#rows, #columns)\n    for i in range(len(class_labels)):\n        ax = fig.add_subplot(gs[i, 0]) # Place plot on a subplot [row,column]\n        ax = sns.countplot(x=class_labels[i]) # For Array, use x=y, to change orientation: \"x=\" → \"y=\"\n        ax.bar_label(ax.containers[0]) # Show number on top of bars\n        ax.title.set_text(f\"Total number of labels: {len(class_labels[i])}\")\n        dummy = ax.set_xticklabels(ax.get_xticklabels(), rotation=90) #If you have too much class, rotates x labels\n\nplot_class_dist(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **6. Image Processing**\n\nNot required, dataset already looks perfect.","metadata":{}},{"cell_type":"markdown","source":"# **7. Target / Label Formatting**","metadata":{}},{"cell_type":"code","source":"classes = np.unique(y, return_counts=True)[0].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(classes)\ny = le.transform(y) # For Classification","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **8. Splitting Data Into Training & Test Sets**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, shuffle = True, random_state = 8)\nx_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size=0.25, shuffle = True, random_state = 8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For one hot decoded labels, y_train -> [label_decoder[y_train[i].argmax()] for i in range(len(y_train))]\ntrain_values, train_counts = np.unique(y_train, return_counts=True) # Directory Iterator: xy_train.classes\nval_values, val_counts = np.unique(y_val, return_counts=True) # Directory Iterator: xy_val.classes\ntest_values, test_counts = np.unique(y_test, return_counts=True) # Directory Iterator: xy_test.classes\n\nfig, ax = plt.subplots(figsize=(23, 7))\npd.DataFrame({'train': train_counts,'val': val_counts, 'test': test_counts},index=classes).plot.bar(ax=ax) \nplt.show() # \"index=classes\" if labels are encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **9. Data Augmentation Of Training Data**","metadata":{}},{"cell_type":"code","source":"import random\nimport copy\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib import pyplot\nimport seaborn as sns\nimport os\nimport cv2\nfrom PIL import Image\nimport albumentations as A\n\nclass ImageAugmentation:\n    def __init__(self):\n        pass\n\n    @classmethod\n    def preprocess(cls, x, bboxes, classes, titles, bbox_format, im_size, num_samples):\n        samples = random.sample(range(len(x)),num_samples) if num_samples !=0 else range(len(x)) # Take required number of samples\n        \n        x = x.values.tolist() if isinstance(x, pd.Series) else x\n        bboxes = bboxes.values if isinstance(bboxes, pd.Series) else bboxes\n        classes = classes.values if isinstance(classes, pd.Series) else classes\n        titles = titles.values if isinstance(titles, pd.Series) else titles\n\n        bboxes = [bboxes[i] for i in samples] if bboxes is not None and num_samples !=0 else bboxes\n        classes = [classes[i] for i in samples] if classes is not None and num_samples !=0 else classes\n        titles = [titles[i] for i in samples] if titles is not None and num_samples !=0 else classes\n\n        if isinstance(x, list):\n            x = np.array([np.array(Image.open(x[i]).convert('RGB')) for i in samples])\n        elif isinstance(x, np.ndarray):\n            x = np.array([x[i] for i in samples]) if num_samples != 0 else x\n        else:\n            print(\"There's a problem with your image data format\")\n            return\n        \n        if im_size is not None:\n            if bboxes is not None and (bbox_format == 'pascal_voc' or bbox_format == 'coco'):\n                bboxes = copy.deepcopy(bboxes)\n                for i,bbox in enumerate(bboxes):\n                    for k,box in enumerate(bbox):\n                        box[0] = box[0]/x[i].shape[1]*im_size[0]\n                        box[1] = box[1]/x[i].shape[0]*im_size[1]\n                        box[2] = box[2]/x[i].shape[1]*im_size[0]\n                        box[3] = box[3]/x[i].shape[0]*im_size[1]\n            x = np.array([cv2.resize(img, im_size) for img in x], dtype=\"uint8\")               \n        return x, bboxes, classes, titles\n\n    @classmethod\n    def just_plot(cls, x, bboxes=None, classes=None, titles=None, bbox_format=None, num_samples=0, im_size=None, num_cols=4, plot_gap=5, mode=\"plot\"):\n        \"\"\"Doesn't make any change, useful for creating array, plotting original images, resizing or copying\n        x: Array containing images or series/list containing image paths\n        bboxes: Array, series or list containing bbox coordinates [[0.12, 0.12, 0.12, 0.12], [0.35, 0.52, 0.62, 0.31]]\n        classes: Array, series or list contraining labels for each bbox [[plate, plate], [plate]]\n        bbox_format: Format of the bboxes. 'pascal_voc', 'albumentations', 'coco' or 'yolo'\n        titles: Array, series or list containing title for each image while plotting (usually filename)\n        \"\"\"\n        x,bboxes,classes, titles = cls.preprocess(x=x, bboxes=bboxes, classes=classes, titles=titles, bbox_format=bbox_format, num_samples=num_samples, im_size=im_size)\n        return cls._plot_or_return(x=x, bboxes=bboxes, classes=classes, titles=titles, bbox_format=bbox_format, num_cols=num_cols, plot_gap=plot_gap, mode=mode)\n\n    @classmethod\n    def _plot_or_return(cls, x, bboxes=None, classes=None, bbox_format=None, titles=None, num_cols=4, plot_gap=5, mode=\"plot\"):\n        if bboxes is None:\n            if mode == \"plot\":\n                titles = [\"\"] * len(x) if titles[0]=='' else titles\n                plt.figure(figsize=(23,len(x)/num_cols*plot_gap))\n                for i in range(len(x)):\n                    plt.subplot(int(len(x)/num_cols)+1,num_cols,i+1)\n                    plt.title(titles[i])\n                    plt.imshow(x[i])\n                plt.tight_layout()\n                plt.show()\n            elif mode == \"return\":\n                return np.array(x, dtype=\"uint8\")\n        else:\n            if classes is not None:\n                bbox_labels = np.array([le.inverse_transform(i) for i in classes])\n                bbox_labels = [el.tolist() if el.size > 0 else '' for el in bbox_labels]\n            else:\n                bbox_labels=None\n            if mode=='plot':\n                cls.plot_bboxes(x=x, y=bboxes, bbox_labels=bbox_labels, titles=titles, bbox_format=bbox_format, num_cols=num_cols, plot_gap=plot_gap)\n            elif mode=='return':\n                return np.array(x), bboxes, classes, titles\n    \n    @classmethod\n    def save(cls, x, y=None, z=None, bbox_format=None, folder_name=\"dataset\", sub_folder_name=\"images\", im_size=None, num_samples=0):\n        \"\"\"\n        Save images in given folder\n        x: Array containing images or dataframe column containing image paths\n        y: Array, list or DF column containing labels for each image (save images for each label folder)\n        z: Array, list or DF column containing names for each image (including format)\n        folder_name: Name of the folder to be created in current directory to save the images\n        sub_folder_name: Name of the folder to be created in folder_name to save the images\n        im_size= if specified as (width,height), resize image\n        \"\"\"\n        x,bboxes,y,z = cls.preprocess(x=x, bboxes=None, classes=y, titles=z, bbox_format=bbox_format, num_samples=num_samples, im_size=im_size)\n\n        if sub_folder_name==0:\n            if not os.path.exists(folder_name):os.mkdir(folder_name)\n        else:\n            if not os.path.exists(folder_name): os.mkdir(folder_name)\n            if not os.path.exists(os.path.join(folder_name,sub_folder_name)): os.mkdir(os.path.join(folder_name,sub_folder_name))\n            folder_name = os.path.join(folder_name,sub_folder_name)\n\n        for i in range(len(x)):\n            if y is None and z is None: # both z and y missing\n                try:\n                    pyplot.imsave(os.path.join(folder_name,\"image %d.png\" %i), x[i])\n                except:\n                    print(\"%s could not be copied\" % os.path.join(folder_name,\"image %d.png\" %i))\n            elif z is None: # only y exist\n                if not os.path.exists(os.path.join(folder_name,y[i])): os.mkdir(os.path.join(folder_name,y[i]))\n                try:\n                    pyplot.imsave(os.path.join(folder_name,y[i],\"image %d.png\" %i), x[i])\n                except:\n                    print(\"%s could not be copied\" % os.path.join(folder_name, y[i],\"image %d.png\" %i))\n            elif y is None: # only z exist\n                try:\n                    pyplot.imsave(os.path.join(folder_name,z[i]), x[i])\n                except:\n                    print(\"%s could not be copied\" % os.path.join(folder_name,z[i]))\n            else: # both z and y exist\n                if not os.path.exists(os.path.join(folder_name,y[i])): os.mkdir(os.path.join(folder_name,y[i]))\n                try:\n                    pyplot.imsave(os.path.join(folder_name,y[i],z[i]), x[i])\n                except:\n                    print(\"%s could not be copied\" % os.path.join(folder_name,y[i],z[i]))\n\n    @classmethod\n    def plot_augmentations_for_single_image(cls, x, bboxes=None, classes=None, titles=None, bbox_format=None, im_size=None, num_cols=4, plot_gap=5, mode=\"plot\"):\n        \"\"\" This function Plot augmentations for single image to pick best augmentations for dataset. \n        If bboxes and classes None, plain images will be plotted.\n        x: Array containing images or Series/list containing image paths\n        bboxes: Series, array or list containing bbox coordinates. [[0.48, 0.53, 0.35, 0.19],[0.48, 0.53, 0.35, 0.19]]\n        classes: Series, array pr list containing class of each bbox. [0, 1]\n        bbox_format: Format of the bboxes. 'pascal_voc', 'albumentations', 'coco' or 'yolo'.\n        im_size= if specified as (width,height), resize image\n        num_cols: Column size while plotting. Set it 2 or 3 to see more details\n        plot_gap: To increase image size or if images nest together, increase plot_gap size\n        \"\"\"\n        x,bboxes,classes, titles = cls.preprocess(x=x, bboxes=bboxes, classes=classes, titles=titles, bbox_format=bbox_format, num_samples=1, im_size=im_size)\n        chosen_image = x[0].copy()\n        albumentation_list = [\n            A.Affine(always_apply=False, p=1, cval=(255, 255, 255)), \n            A.AdvancedBlur(always_apply=False, p=1, blur_limit=(3, 13), sigmaX_limit=(0.2, 1.0), sigmaY_limit=(0.2, 1.0), rotate_limit=90, beta_limit=(0.5, 8.0), noise_limit=(0.9, 1.1)), \n            A.Blur(always_apply=False, p=1.0, blur_limit=(5, 10)), \n            A.ChannelDropout(always_apply=False, p=1.0, channel_drop_range=(1, 2), fill_value=0), \n            A.ChannelShuffle(always_apply=False, p=1.0), \n            A.CLAHE(always_apply=False, p=1.0, clip_limit=(13, 21), tile_grid_size=(23, 10)), \n            A.ColorJitter(always_apply=False, p=1.0, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), \n            A.CropAndPad(p=1, percent=0.1), \n            A.Downscale(always_apply=False, p=1.0, scale_min=0.15, scale_max=0.45, interpolation=0), \n            A.Equalize(always_apply=False, p=1.0, mode='cv', by_channels=True), \n            A.FancyPCA(always_apply=False, p=1.0, alpha=0.1), \n            A.Flip(always_apply=False, p=1), # Flip either horizontally, vertically or both horizontally and vertically. \n            A.GaussNoise(always_apply=False, p=1.0, var_limit=(150.0, 350.0), per_channel=True, mean=0.0), \n            A.GaussianBlur(always_apply=False, p=1.0, blur_limit=(3, 7), sigma_limit=0), \n            A.GlassBlur(always_apply=False, p=1.0, sigma=0.7, max_delta=4, iterations=2), \n            A.HorizontalFlip(always_apply=False, p=1), \n            A.HueSaturationValue(always_apply=False, p=1.0, hue_shift_limit=(40, 50), sat_shift_limit=(20, 157), val_shift_limit=(-20, 50)), \n            A.ISONoise(always_apply=False, p=1.0, intensity=(0.1, 0.5), color_shift=(0.25, 0.85)), \n            A.MedianBlur(always_apply=False, p=1.0, blur_limit=7), \n            A.MotionBlur(always_apply=False, p=1.0, blur_limit=(25, 33)), \n            A.MultiplicativeNoise(always_apply=False, p=1.0, multiplier=(0.6, 1.7), per_channel=True, elementwise=True), \n            A.Perspective(always_apply=False, p=1.0, scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1), \n            A.PiecewiseAffine(always_apply=False, p=1), \n            A.PixelDropout(always_apply=False, p=1, dropout_prob=0.05, drop_value=0), \n            A.Posterize(always_apply=False, p=1.0, num_bits=[(0, 5), (0, 8), (0, 6)]), \n            A.RandomBrightness(always_apply=False, p=1.0, limit=(-0.4, 0.5)), \n            A.RandomBrightnessContrast(always_apply=False, p=1.0, brightness_limit=(-0.4, 0.5), contrast_limit=(-0.4, 0.5), brightness_by_max=True), \n            A.RandomContrast(always_apply=False, p=1.0, limit=(-0.9, 0.9)), \n            A.RandomCrop(always_apply=False, p=1, height=200, width=200), \n            A.RandomFog(always_apply=False, p=1.0, fog_coef_lower=0.22, fog_coef_upper=0.38, alpha_coef=1.0), \n            A.RandomGamma(always_apply=False, p=1.0, gamma_limit=(35, 150), eps=None), \n            A.RandomRain(always_apply=False, p=1.0, slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(0, 0, 0), blur_value=4, brightness_coefficient=0.5, rain_type=None), \n            A.RandomResizedCrop(always_apply=False, p=1.0, height=150, width=150, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=0), \n            A.RandomRotate90(always_apply=False, p=1.0), \n            A.RandomShadow(always_apply=False, p=1.0, shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, shadow_dimension=5), \n            A.RandomSnow(always_apply=False, p=1.0, snow_point_lower=0.1, snow_point_upper=0.8, brightness_coeff=2.5), \n            A.RandomSunFlare(always_apply=False, p=1), \n            A.RandomToneCurve(always_apply=False, p=1, scale=0.8), \n            A.Resize(always_apply=False, p=1, height=500, width=500), # You normally don't need, but keep it just in case \n            A.RGBShift(always_apply=False, p=1.0, r_shift_limit=(29, 20), g_shift_limit=(-20, 150), b_shift_limit=(-20, 150)), \n            A.RingingOvershoot(always_apply=False, p=1.0, blur_limit=(7, 15), cutoff=(0.7853981633974483, 1.5707963267948966)), \n            A.Rotate(always_apply=False, p=1.0, limit=(-90, 90), rotate_method='largest_box', crop_border=False), \n            A.SafeRotate(always_apply=False, p=1), \n            A.Sharpen(always_apply=False, p=1, alpha=(0.2, 0.5), lightness=(0.5, 1.0)),\n            A.ShiftScaleRotate(always_apply=False, p=1.0, rotate_limit=(-90, 90),rotate_method='largest_box'),\n            A.Solarize(always_apply=False, p=1.0, threshold=(150, 150)), \n            A.Superpixels(always_apply=False, p=1.0, p_replace=0.1, n_segments=100, max_size=128, interpolation=1), \n            A.Transpose(always_apply=False, p=1.0), \n            A.ToGray(always_apply=False, p=1.0), # Might come in handy later, don't use! \n            A.ToSepia(always_apply=False, p=1.0), \n            A.VerticalFlip(always_apply=False, p=1.0)]\n        \n        bbox_incompatible_albumentation_list = [\n            A.CenterCrop(always_apply=False, p=1.0, height=150, width=150), \n            A.Crop(always_apply=False, p=1.0, x_min=0, y_min=0, x_max=200, y_max=200), \n            A.CoarseDropout(always_apply=False, p=1.0, max_holes=50, max_height=15, max_width=15, min_holes=None, min_height=None, min_width=None, fill_value=0), \n            A.ElasticTransform(always_apply=False, p=1.0, alpha=1, sigma=50, alpha_affine=50, interpolation=1, border_mode=4, value=None, mask_value=None),\n            A.GridDistortion(always_apply=False, p=1.0, num_steps=5, distort_limit=0.8, interpolation=1, border_mode=4), \n            A.GridDropout(always_apply=False, p=1.0, ratio=0.5, unit_size_min=10, unit_size_max=10, holes_number_x=10, holes_number_y=10, shift_x=0, shift_y=0, random_offset=False, fill_value=0, mask_fill_value=None),\n            A.OpticalDistortion(always_apply=False, p=1.0, distort_limit=0.4, shift_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None),\n            A.RandomGridShuffle(always_apply=False, p=1.0, grid=(3, 3))]\n      \n        titles_list = [\"Original\",\"Affine\", \"AdvancedBlur\", \"Blur\",\"ChannelDropout\", \n               \"ChannelShuffle\", \"CLAHE\", \"ColorJitter\",\"CropAndPad\",  \n               \"Downscale\",\"Equalize\", \"FancyPCA\", \"Flip\", \"GaussNoise\",  \n               \"GaussianBlur\", \"GlassBlur\",\"HorizontalFlip\",  \n               \"HueSaturationValue\", \"ISONoise\", \"MedianBlur\",\"MotionBlur\",\"MultiplicativeNoise\",  \n               \"Perspective\",\"PiecewiseAffine\", \"PixelDropout\", \"Posterize\", \n               \"RandomBrightness\", \"RandomBrightnessContrast\", \"RandomContrast\",\"RandomCrop\",  \n               \"RandomFog\", \"RandomGamma\",\"RandomRain\", \"RandomResizedCrop\",  \n               \"RandomRotate90\", \"RandomShadow\", \"RandomSnow\", \"RandomSunFlare\", \"RandomToneCurve\", \n               \"Resize\", \"RGBShift\", \"Rotate\", \"RingingOvershoot\", \"SafeRotate\", \"Sharpen\", \"ShiftScaleRotate\",  \n               \"Solarize\", \"Superpixels\",\"Transpose\", \"ToGray\", \"ToSepia\", \"VerticalFlip\" ]\n        \n        bbox_incompatible_titles_list = ['CenterCrop', 'Crop', 'CoarseDropout', 'ElasticTransform',\n                           'GridDistortion', 'GridDropout', 'OpticalDistortion', 'RandomGridShuffle']\n\n        x_aug,bboxes_aug,classes_aug = [],[],[]\n        \n        if bboxes is None and classes is None:\n            titles_list.extend(bbox_incompatible_titles_list)\n            albumentation_list.extend(bbox_incompatible_albumentation_list)\n            for aug_type in albumentation_list:\n                transform = A.Compose([aug_type])\n                transformed = transform(image = chosen_image)\n                if np.isnan(transformed['image']).any() or transformed['image'].max() > 255 or transformed['image'].min() < 0:\n                    x_aug.append(chosen_image)\n                else:\n                    x_aug.append(transformed['image'])\n            x_aug.insert(0,chosen_image) #insert original image at the beginning\n            if im_size is not None: x_aug = np.array([cv2.resize(x_aug[i], im_size) for i in range(len(x_aug))], dtype=\"uint8\")\n            return cls._plot_or_return(x=x_aug, titles=titles_list, num_cols=num_cols, plot_gap=plot_gap, mode=mode)\n        elif bboxes is not None and classes is not None:\n            chosen_bbox,chosen_id = bboxes[0].copy(),classes[0].copy()\n            for aug_type in albumentation_list:\n                transform = A.Compose([aug_type], bbox_params=A.BboxParams(format=bbox_format, label_fields=['category_ids'], min_visibility=0.3))\n                transformed = transform(image = chosen_image, bboxes=chosen_bbox, category_ids=chosen_id)\n                if np.isnan(transformed['image']).any() or transformed['image'].max() > 255 or transformed['image'].min() < 0:\n                    x_aug.append(chosen_image) \n                else:\n                    x_aug.append(transformed['image'])\n                    bboxes_aug.append(list(transformed['bboxes']))\n                    classes_aug.append(transformed['category_ids'])\n            x_aug.insert(0,chosen_image)\n            bboxes_aug.insert(0,chosen_bbox)\n            classes_aug.insert(0,chosen_id)\n            return cls._plot_or_return(x=x_aug, bboxes=bboxes_aug, classes=classes_aug, titles=titles_list, bbox_format=bbox_format, num_cols=num_cols, plot_gap=plot_gap, mode=mode)\n\n    @classmethod\n    def augment_images(cls, x, bboxes=None, classes=None, titles=None, bbox_format=None, im_size=None, num_samples=0, num_cols=2, plot_gap=6, mode=\"plot\"):\n        \"\"\"\n        Create new batch from dataset after applying augmentation\n        x: Array containing images or dataframe column containing image paths\n        im_size= if specified as (width,height), resize image\n        num_samples: If x is too large, you may only crop & plot few random samples\n        num_cols: Column size while plotting. Set it 2 or 3 to see more details\n        plot_gap: To increase image size or if images nest together, increase plot_gap size\n        mode: plot → plot samples to find best parameters | return → returns new array with cropped imgs\n        \"\"\"\n        x,bboxes,classes,titles = cls.preprocess(x=x, bboxes=bboxes, classes=classes, titles=titles, bbox_format=bbox_format, num_samples=num_samples, im_size=im_size)\n        \n        albumentation_list = [\n            A.Affine(always_apply=False, p=0.5, cval=(255, 255, 255)), \n            A.AdvancedBlur(always_apply=False, p=0.5, blur_limit=(3, 13), sigmaX_limit=(0.2, 1.0), sigmaY_limit=(0.2, 1.0), rotate_limit=90, beta_limit=(0.5, 8.0), noise_limit=(0.9, 1.1)), \n            A.Blur(always_apply=False, p=0.5, blur_limit=(5, 10)), \n            A.ChannelDropout(always_apply=False, p=0.5, channel_drop_range=(1, 2), fill_value=0), \n            A.ChannelShuffle(always_apply=False, p=0.5), \n            A.CLAHE(always_apply=False, p=0.5, clip_limit=(13, 21), tile_grid_size=(23, 10)), \n            A.ColorJitter(always_apply=False, p=0.5, brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2), \n            #A.CropAndPad(p=0.5, percent=0.1), \n            A.Downscale(always_apply=False, p=0.5, scale_min=0.15, scale_max=0.45, interpolation=0), \n            A.Equalize(always_apply=False, p=0.5, mode='cv', by_channels=True), \n            A.FancyPCA(always_apply=False, p=0.5, alpha=0.1), \n            A.Flip(always_apply=False, p=0.5), # Flip either horizontally, vertically or both horizontally and vertically. \n            A.GaussNoise(always_apply=False, p=0.5, var_limit=(150.0, 350.0), per_channel=True, mean=0.0), \n            A.GaussianBlur(always_apply=False, p=0.5, blur_limit=(3, 7), sigma_limit=0), \n            A.GlassBlur(always_apply=False, p=0.5, sigma=0.7, max_delta=4, iterations=2), \n            A.HorizontalFlip(always_apply=False, p=0.5), \n            A.HueSaturationValue(always_apply=False, p=0.5, hue_shift_limit=(40, 50), sat_shift_limit=(20, 157), val_shift_limit=(-20, 50)), \n            A.ISONoise(always_apply=False, p=0.5, intensity=(0.1, 0.5), color_shift=(0.25, 0.85)), \n            A.MedianBlur(always_apply=False, p=0.5, blur_limit=7), \n            A.MotionBlur(always_apply=False, p=0.5, blur_limit=(25, 33)), \n            A.MultiplicativeNoise(always_apply=False, p=0.5, multiplier=(0.6, 1.7), per_channel=True, elementwise=True), \n            A.Perspective(always_apply=False, p=0.5, scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1), \n            A.PiecewiseAffine(always_apply=False, p=0.5), \n            A.PixelDropout(always_apply=False, p=0.5, dropout_prob=0.05, drop_value=0), \n            A.Posterize(always_apply=False, p=0.5, num_bits=[(0, 5), (0, 8), (0, 6)]), \n            A.RandomBrightness(always_apply=False, p=0.5, limit=(-0.4, 0.5)), \n            A.RandomBrightnessContrast(always_apply=False, p=0.5, brightness_limit=(-0.4, 0.5), contrast_limit=(-0.4, 0.5), brightness_by_max=True), \n            A.RandomContrast(always_apply=False, p=0.5, limit=(-0.9, 0.9)), \n            #A.RandomCrop(always_apply=False, p=0.5, height=200, width=200), \n            A.RandomFog(always_apply=False, p=0.5, fog_coef_lower=0.22, fog_coef_upper=0.38, alpha_coef=1.0), \n            A.RandomGamma(always_apply=False, p=0.5, gamma_limit=(35, 150), eps=None), \n            A.RandomRain(always_apply=False, p=0.5, slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(0, 0, 0), blur_value=4, brightness_coefficient=0.5, rain_type=None), \n            A.RandomResizedCrop(always_apply=False, p=0.5, height=150, width=150, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=0), \n            A.RandomRotate90(always_apply=False, p=0.5), \n            A.RandomShadow(always_apply=False, p=0.5, shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, shadow_dimension=5), \n            A.RandomSnow(always_apply=False, p=0.5, snow_point_lower=0.1, snow_point_upper=0.8, brightness_coeff=2.5), \n            A.RandomSunFlare(always_apply=False, p=0.5), \n            A.RandomToneCurve(always_apply=False, p=0.5, scale=0.8), \n            #A.Resize(always_apply=False, p=0.5, height=500, width=500), # You normally don't need, but keep it just in case \n            A.RGBShift(always_apply=False, p=0.5, r_shift_limit=(29, 20), g_shift_limit=(-20, 150), b_shift_limit=(-20, 150)), \n            A.RingingOvershoot(always_apply=False, p=0.5, blur_limit=(7, 15), cutoff=(0.7853981633974483, 1.5707963267948966)), \n            A.Rotate(always_apply=False, p=0.5, limit=(-90, 90), rotate_method='largest_box', crop_border=False), \n            A.SafeRotate(always_apply=False, p=0.5), \n            A.Sharpen(always_apply=False, p=0.5, alpha=(0.2, 0.5), lightness=(0.5, 1.0)), \n            A.ShiftScaleRotate(always_apply=False, p=0.5, rotate_limit=(-90, 90),rotate_method='largest_box'), \n            A.Solarize(always_apply=False, p=0.5, threshold=(150, 150)), \n            A.Superpixels(always_apply=False, p=0.5, p_replace=0.1, n_segments=100, max_size=128, interpolation=1), \n            A.Transpose(always_apply=False, p=0.5), \n            A.ToGray(always_apply=False, p=0.5), # Might come in handy later, don't use! \n            A.ToSepia(always_apply=False, p=0.5), \n            A.VerticalFlip(always_apply=False, p=0.5)]\n        \n        bbox_incompatible_albumentation_list = [\n            A.CenterCrop(always_apply=False, p=0.5, height=150, width=150),\n            A.Crop(always_apply=False, p=0.5, x_min=0, y_min=0, x_max=200, y_max=200),\n            A.CoarseDropout(always_apply=False, p=0.5, max_holes=50, max_height=15, max_width=15, min_holes=None, min_height=None, min_width=None, fill_value=0), \n            A.ElasticTransform(always_apply=False, p=0.5, alpha=1, sigma=50, alpha_affine=50, interpolation=1, border_mode=4, value=None, mask_value=None),\n            A.GridDistortion(always_apply=False, p=0.5, num_steps=5, distort_limit=0.8, interpolation=1, border_mode=4), \n            A.GridDropout(always_apply=False, p=0.5, ratio=0.5, unit_size_min=10, unit_size_max=10, holes_number_x=10, holes_number_y=10, shift_x=0, shift_y=0, random_offset=False, fill_value=0, mask_fill_value=None),\n            A.OpticalDistortion(always_apply=False, p=0.5, distort_limit=0.4, shift_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None),\n            A.RandomGridShuffle(always_apply=False, p=0.5, grid=(3, 3))]\n        \n        x_aug,bboxes_aug,classes_aug = [],[],[]\n        \n        if bboxes is None and classes is None:\n            albumentation_list.extend(bbox_incompatible_albumentation_list)\n            for i in range(len(x)):\n                transform = A.Compose([A.SomeOf(albumentation_list, 3, replace=True, p=1)])\n                transformed = transform(image = x[i].copy())\n                if np.isnan(transformed['image']).any() or transformed['image'].max() > 255 or transformed['image'].min() < 0:\n                    x_aug.append(x[i])\n                else:\n                    x_aug.append(transformed['image'])\n                    \n            if im_size[0]!=\"\": x_aug = np.array([cv2.resize(x_aug[i], im_size) for i in range(len(x_aug))], dtype=\"uint8\")\n                \n            return cls._plot_or_return(x=x_aug, num_cols=num_cols, plot_gap=plot_gap, mode=mode)\n        elif bboxes is not None and classes is not None:\n            if len(bboxes) != len(classes) or len(classes) != len(x): return print('You should have same #images, #bboxes and #classes')\n            for i in range(len(x)):\n                transform = A.Compose([A.SomeOf(albumentation_list, 3, replace=True, p=1)], bbox_params=A.BboxParams(format=bbox_format, label_fields=['category_ids'], min_visibility=0.3))\n                transformed = transform(image = x[i].copy(),bboxes=bboxes[i].copy(),category_ids=classes[i].copy())\n                if np.isnan(transformed['image']).any() or transformed['image'].max() > 255 or transformed['image'].min() < 0:\n                    x_aug.append(x[i])\n                    bboxes_aug.append([''])\n                    classes_aug.append([''])\n                else:\n                    x_aug.append(transformed['image'])\n                    bboxes_aug.append(list(transformed['bboxes']))\n                    classes_aug.append(transformed['category_ids'])\n            return cls._plot_or_return(x=x_aug, bboxes=bboxes_aug, classes=classes_aug, titles=titles, bbox_format=bbox_format, num_cols=num_cols, plot_gap=plot_gap, mode=mode)\n\n    @classmethod\n    def plot_bboxes(cls, x, y, bbox_labels=None, titles=None, bbox_format=\"albumentations\", num_cols=3, plot_gap=2): \n        \"\"\" \n        Plot images with bboxes from array or dataframe \n        x: Image array generated by cv2.imread(\"img.jpg\")[..., ::-1] or image_path column of dataframe \n        y: Array or dataframe column containing bbox coordinates  \n        bbox_labels: array or dataframe column containing labels for each bboxes \n        indexes: indexes of the images as a list you want to plot (e.g tinybox_indexes or [1, 64, 53]) \n        titles: array or dataframe column containing title for each image while plotting(usually filename)\n        bbox_format: \"albumentations\", \"pascal_voc\", \"coco\" or \"yolo\" based on bbox format of your data  \n        \"\"\" \n        # Turn filenames into desired list format \n        titles = [\"\"] * len(x) if titles is None else titles\n        \n        width,height = [a.shape[1] for a in x], [a.shape[0] for a in x]\n        \n        plt.figure(figsize=(20,len(x)/num_cols*plot_gap)) # Increase plot_gap if images nested together\n        for i,bboxes in enumerate(y): \n            img = x[i] if x[i].max() <= 1 else x[i]/255 \n            if len(bboxes) == 0:\n                plt.subplot(int(len(x)/num_cols)+1,num_cols,i+1) \n                plt.title(str(titles[i]) + \"\\n\" + str(img.shape)) \n                plt.imshow(img) \n                continue \n            else:\n                for k, bbox in enumerate(bboxes): \n                    if bbox_format==\"albumentations\": \n                        [x1,y1,x2,y2] = [bbox[0]*width[i], bbox[1]*height[i], bbox[2]*width[i], bbox[3]*height[i]] \n                        [x1,y1,x2,y2] = [int(x) for x in [x1,y1,x2,y2]] \n                    elif bbox_format==\"pascal_voc\": \n                        [x1,y1,x2,y2] = bbox \n                        [x1,y1,x2,y2] = [int(x) for x in [x1,y1,x2,y2]] \n                    if bbox_format==\"yolo\": \n                        xcenter, ycenter, w, h = bbox\n                        x1 = int((xcenter - w / 2) * width[i])\n                        x2 = int((xcenter + w / 2) * width[i])\n                        y1 = int((ycenter - h / 2) * height[i])\n                        y2 = int((ycenter + h / 2) * height[i])\n                        if x1 < 0: x1 = 0\n                        if x2 > width[i] - 1: x2 = width[i] - 1\n                        if y1 < 0: y1 = 0\n                        if y2 > height[i] - 1: y1 = height[i] - 1\n                    if bbox_format==\"coco\":\n                        x1, y1, w, h = bbox\n                        x2, y2 = x1 + w, y1 + h\n                        [x1,y1,x2,y2] = [int(x) for x in [x1,y1,x2,y2]]\n                    plt.subplot(int(len(x)/num_cols)+1,num_cols,i+1)\n                    cv2.rectangle(img, (x1,y1), (x2,y2), (255,0,0), 2)\n                    if bbox_labels is not None: cv2.putText(img, bbox_labels[i][k], (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,0,0), 4)\n                    plt.title(str(titles[i]) + \"\\n\" + str(img.shape))\n                plt.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ImageAugmentation.plot_augmentations_for_single_image(x, im_size=(224,224))\n# Based on plotted augmentations, I select these augmentations\n#affine\n# advancedBlur\n# Blur\n# Downscale\n# GaussNoise\n# Gaussianblur\n#glassblur\n# multicaptivenoise\n# perspective\n# piecewiseaffine\n# pixeldropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply Albumentations augmentation\nimport albumentations as A\ndef albumentations_aug(image):\n    aug = A.Compose([\n        A.SomeOf([\n            A.Affine(always_apply=False, p=0.5, cval=(255, 255, 255)), \n            A.AdvancedBlur(always_apply=False, p=0.5, blur_limit=(3, 13), sigmaX_limit=(0.2, 1.0), sigmaY_limit=(0.2, 1.0), rotate_limit=90, beta_limit=(0.5, 8.0), noise_limit=(0.9, 1.1)), \n            A.Blur(always_apply=False, p=0.5, blur_limit=(5, 10)), \n            A.Downscale(always_apply=False, p=0.5, scale_min=0.15, scale_max=0.45, interpolation=0), \n            A.GaussNoise(always_apply=False, p=0.5, var_limit=(150.0, 350.0), per_channel=True, mean=0.0), \n            A.GaussianBlur(always_apply=False, p=0.5, blur_limit=(3, 7), sigma_limit=0), \n            A.GlassBlur(always_apply=False, p=0.5, sigma=0.7, max_delta=4, iterations=2), \n            A.MultiplicativeNoise(always_apply=False, p=0.5, multiplier=(0.6, 1.7), per_channel=True, elementwise=True), \n            A.Perspective(always_apply=False, p=0.5, scale=(0.05, 0.1), keep_size=True, pad_mode=0, pad_val=0, mask_pad_val=0, fit_output=False, interpolation=1), \n            A.PiecewiseAffine(always_apply=False, p=0.5), \n            A.PixelDropout(always_apply=False, p=0.5, dropout_prob=0.05, drop_value=0), \n            ], 5, replace=True, p=1)\n            ])\n    return aug(image=image)['image']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n# Loading + Augmentation Function For Training Data \ndata_generator = ImageDataGenerator(\n        rescale=1./255, # Normalize pixels while loading\n        featurewise_center=False, # set input mean to 0 over the dataset\n        samplewise_center=False, # set each sample mean to 0\n        featurewise_std_normalization=False, # divide inputs by std of the dataset\n        samplewise_std_normalization=False, # divide each input by its std\n        zca_whitening=False, # apply ZCA whitening\n) # preprocessing_function = albumentations_aug","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_generator = ImageDataGenerator(\n        rescale=1./255) # Normalize pixels while loading ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xy_train = data_generator.flow(\n    x_train,\n    y_train,\n    batch_size=16,\n    shuffle=True, # (default: True) For training data, it must be True\n    seed=16,\n) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load (x_val, y_val) YOU DON'T ACTUALLY NEED THIS FOR VALIDATION DATA, JUST USE x_val, y_val\nxy_val = val_generator.flow(\n    x_val,\n    y_val,\n    batch_size=16,\n    shuffle=True, # (default: True) For validation data, it can be True or False – True preffered\n    seed=16) # Only used if validation_split is set !!!","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **10. Feature Scaling For Training, Test & Validation Features (After Splitting & Augmentation)**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}